---
layout: single
title:  "Day  11: Chart & Weather Data Crawling "
categories: [study-logs]
tag: [python, blog, html, data]
toc: true
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>



```python
from bs4 import BeautifulSoup
import re

html = """<html> <head><title>test site</title></head> <body> <div><p id="i" 
class="a">test1</p><p class="d">test2</p></div><p class="d">test3</p></p> <a 
href="/example/test1">a tag</a> <b>b tag</b></body></html>"""

soup = BeautifulSoup(html, 'lxml')


print(soup.find_all(class_=re.compile('d')))   # 클래스에 d가 포함된 모든 요소 가져오기
print(soup.find_all(id=re.compile('i')))      # 아이디에 i가 포함된 모든 요소 가져오기
print(soup.find_all(re.compile('t')))         # 태그에 t가 포함된 모든 요소 가져오기
print(soup.find_all(re.compile('^t')))        # 태그가 t로 시작하는 모든 요소 가져오기
print(soup.find_all(href=re.compile('/')))    # href에 슬래시(/)가 포함된 모든 요소 가져오기
```

<pre>
[<p class="d">test2</p>, <p class="d">test3</p>]
[<p class="a" id="i">test1</p>]
[<html> <head><title>test site</title></head> <body> <div><p class="a" id="i">test1</p><p class="d">test2</p></div><p class="d">test3</p> <a href="/example/test1">a tag</a> <b>b tag</b></body></html>, <title>test site</title>]
[<title>test site</title>]
[<a href="/example/test1">a tag</a>]
</pre>

```python
#전화번호 추출
test_num = "저의 전화번호는 010-6666-7777 입니다"

pattern = re.compile('[0-9][0-9][0-9]-[0-9][0-9][0-9][0-9]-[0-9][0-9][0-9][0-9]')
pattern1 = re.compile('\d\d\d-\d\d\d\d-\d\d\d\d')
pattern2 = re.compile('\d{3}-\d{4}-\d{4}')
c= pattern.findall(test_num)
d = pattern1.findall(test_num)
e = pattern2.findall(test_num)
print('==== 전화번호 찾기 ====')
print(c)
print(d)
print(e)
```

<pre>
==== 전화번호 찾기 ====
['010-6666-7777']
['010-6666-7777']
['010-6666-7777']
</pre>
### Raw 문자열 : r'문자열' (해당 문자열은 정규표현식이야)

### 역슬래시 두번 사용하기 \\



```python
import requests
from bs4 import BeautifulSoup
#codes 작업

url = "https://finance.naver.com/item/main.naver?code=" + '005930'
response = requests.get(url)

if response.status_code == 200:
    html = response.text
    soup = BeautifulSoup(html, 'lxml')
    fav = soup.select( 'div#aside div#fav_kospi table.rank a')
    
    pattern = re.compile(r'code=(\d+)')
   
    codes = [pattern.findall(a['href'])[0] 
    for a in fav if pattern.findall(a['href'])]

    #codes = [a['href'].split('code=')[-1] for a in fav if pattern.findall(a['href'])] # [-6:] 슬라이싱은 좋지 않다.
    print (codes)
else:
    print (response.status_code)
```

<pre>
['005930', '042660', '066970', '010140', '035720', '034020', '001570', '005490', '272210', '000660']
</pre>

```python
```


```python
base_url ='https://music.bugs.co.kr/chart/track/week/total?chartdate=20250311'

#response = requests.get(base_url)
#html = response.text
html = requests.get(base_url).text    #이렇게 한번에 가능
soup = BeautifulSoup(html,'lxml')

titles = soup.select('p.title a')
artists = soup.select('p.artist a:not(.more)')   # a:not(.more): a태그 more 빼고 가져와줘

#music_titles = [title.get_text(strip=True) for title in titles]
#music_artists = [artist.get_text(strip=True) for artist in artists]
music_titles = [title.text for title in titles]
music_artists = [artist.text for artist in artists]

  
#print(music_titles, music_artists)

#{title} : {artist} 이런 형태로도 출력해보쟈!
#zip은 list 2개 묶어줌
#enumerate 는 숫자 묶어줌 enumerate(iterable, start =)
for rank, (title, artist) in enumerate(zip(music_titles, music_artists),start =1):
    print(f"{rank}.{title} : {artist}")

```

<pre>
1.TOO BAD (feat. Anderson .Paak) : G-DRAGON
2.I DO ME : KiiiKiii (키키)
3.REBEL HEART : IVE (아이브)
4.ATTITUDE : IVE (아이브)
5.Whiplash : aespa
6.Drowning : WOODZ
7.TAKE ME : G-DRAGON
8.나는 반딧불 : 황가람
9.The Chase : Hearts2Hearts (하츠투하츠)
10.HOME SWEET HOME (feat. 태양, 대성) : G-DRAGON
11.웃기는 소리 같겠지만 : 황치열
12.toxic till the end : 로제(ROSÉ)
13.APT. : 로제(ROSÉ)
14.ExtraL (feat. Doechii) : 제니 (JENNIE)
15.like JENNIE : 제니 (JENNIE)
16.오늘만 I LOVE YOU : BOYNEXTDOOR
17.모르시나요(PROD.로코베리) : 조째즈
18.HAPPY : DAY6 (데이식스)
19.Die With A Smile : Lady GaGa(레이디 가가)
20.luther : Kendrick Lamar(켄드릭 라마)
21.PO￦ER : G-DRAGON
22.earthquake : 지수(JISOO)
23.Love Hangover (feat. Dominic Fike) : 제니 (JENNIE)
24.DRIP : BABYMONSTER
25.Supernova : aespa
26.Mantra : 제니 (JENNIE)
27.한 페이지가 될 수 있게 : DAY6 (데이식스)
28.너와의 모든 지금 : 재쓰비 (JAESSBEE)
29.청춘만화 : 이무진
30.IBELONGIIU : G-DRAGON
31.내게 사랑이 뭐냐고 물어본다면 : 로이킴
32.Welcome to the Show : DAY6 (데이식스)
33.UP (KARINA Solo) : aespa
34.너에게 닿기를 : 10CM
35.내 이름 맑음 : QWER
36.Not Like Us : Kendrick Lamar(켄드릭 라마)
37.고민중독 : QWER
38.DRAMA : G-DRAGON
39.미치게 그리워서 : 황가람
40.number one girl : 로제(ROSÉ)
41.Supernatural : NewJeans
42.How Sweet : NewJeans
43.소나기 : 이클립스 (ECLIPSE)
44.첫 만남은 계획대로 되지 않아 : TWS (투어스)
45.Dangerous : 인피니트(Infinite)
46.천상연 : 이창섭
47.나는 아픈 건 딱 질색이니까 : (여자)아이들
48.에피소드 : 이무진
49.Igloo : KISS OF LIFE
50.Armageddon : aespa
51.예뻤어 : DAY6 (데이식스)
52.녹아내려요 : DAY6 (데이식스)
53.GYRO-DROP : G-DRAGON
54.Magnetic : 아일릿(ILLIT)
55.I AM : IVE (아이브)
56.어떻게 이별까지 사랑하겠어, 널 사랑하는 거지 : AKMU(악뮤)
57.Hype Boy : NewJeans
58.Love wins all : 아이유(IU)
59.무제(無題) (Untitled, 2014) : G-DRAGON
60.Ditto : NewJeans
61.Small girl (feat. 도경수(D.O.)) : 이영지
62.모든 날, 모든 순간 (Every day, Every Moment) : 폴킴(Paul Kim)
63.Supersonic : 프로미스나인
64.Hug (포옹) : RIIZE
65.슬픈 초대장 : 순순희 (지환)
66.Attention : NewJeans
67.밤양갱 : 비비(BIBI)
68.너의 모든 순간 : 성시경
69.숲 : 최유리
70.Love me or Leave me : DAY6 (데이식스)
71.Bubble Gum : NewJeans
72.Sticky : KISS OF LIFE
73.그대만 있다면 (여름날 우리 X 너드커넥션 (Nerd Connection)) : 너드커넥션(Nerd Connection)
74.Letter To Myself : 태연 (TAEYEON)
75.Congratulations : DAY6 (데이식스)
76.해야 (HEYA) : IVE (아이브)
77.비의 랩소디 : 임재현
78.To. X : 태연 (TAEYEON)
79.주저하는 연인들을 위해 : 잔나비
80.ETA : NewJeans
81.보나마나 (BONAMANA) : G-DRAGON
82.Cherish (My Love) : 아일릿(ILLIT)
83.Cruel Summer : Taylor Swift(테일러 스위프트)
84.우리의 다정한 계절 속에 (Season of Memories) : 여자친구(GFRIEND)
85.청혼하지 않을 이유를 못 찾았어 : 이무진
86.인사 : 범진
87.LOVE DIVE : IVE (아이브)
88.Handlebars (feat. Dua Lipa) : 제니 (JENNIE)
89.Live My Life : aespa
90.사랑인가 봐 : 멜로망스(MeloMance)
91.After LIKE : IVE (아이브)
92.사건의 지평선 : 윤하(Younha/ユンナ)
93.헤어지자 말해요 : 박재정
94.SPOT! (feat. JENNIE) : 지코 (ZICO)
95.미안해 미워해 사랑해 : Crush
96.청바지 : 부석순(SEVENTEEN)
97.네모네모 : YENA (최예나)
98.Boom Boom Bass : RIIZE
99.ZEN : 제니 (JENNIE)
100.LOVE ATTACK : RESCENE (리센느)
</pre>

```python
year= 2025
month=1
day=1
bugs_music_week(year, month , day)
```

<pre>
(['TOO BAD (feat. Anderson .Paak)',
  'I DO ME',
  'REBEL HEART',
  'ATTITUDE',
  'Whiplash',
  'Drowning',
  'TAKE ME',
  '나는 반딧불',
  'The Chase',
  'HOME SWEET HOME (feat. 태양, 대성)',
  '웃기는 소리 같겠지만',
  'toxic till the end',
  'APT.',
  'ExtraL (feat. Doechii)',
  'like JENNIE',
  '오늘만 I LOVE YOU',
  '모르시나요(PROD.로코베리)',
  'HAPPY',
  'Die With A Smile',
  'luther',
  'PO￦ER',
  'earthquake',
  'Love Hangover (feat. Dominic Fike)',
  'DRIP',
  'Supernova',
  'Mantra',
  '한 페이지가 될 수 있게',
  '너와의 모든 지금',
  '청춘만화',
  'IBELONGIIU',
  '내게 사랑이 뭐냐고 물어본다면',
  'Welcome to the Show',
  'UP (KARINA Solo)',
  '너에게 닿기를',
  '내 이름 맑음',
  'Not Like Us',
  '고민중독',
  'DRAMA',
  '미치게 그리워서',
  'number one girl',
  'Supernatural',
  'How Sweet',
  '소나기',
  '첫 만남은 계획대로 되지 않아',
  'Dangerous',
  '천상연',
  '나는 아픈 건 딱 질색이니까',
  '에피소드',
  'Igloo',
  'Armageddon',
  '예뻤어',
  '녹아내려요',
  'GYRO-DROP',
  'Magnetic',
  'I AM',
  '어떻게 이별까지 사랑하겠어, 널 사랑하는 거지',
  'Hype Boy',
  'Love wins all',
  '무제(無題) (Untitled, 2014)',
  'Ditto',
  'Small girl (feat. 도경수(D.O.))',
  '모든 날, 모든 순간 (Every day, Every Moment)',
  'Supersonic',
  'Hug (포옹)',
  '슬픈 초대장',
  'Attention',
  '밤양갱',
  '너의 모든 순간',
  '숲',
  'Love me or Leave me',
  'Bubble Gum',
  'Sticky',
  '그대만 있다면 (여름날 우리 X 너드커넥션 (Nerd Connection))',
  'Letter To Myself',
  'Congratulations',
  '해야 (HEYA)',
  '비의 랩소디',
  'To. X',
  '주저하는 연인들을 위해',
  'ETA',
  '보나마나 (BONAMANA)',
  'Cherish (My Love)',
  'Cruel Summer',
  '우리의 다정한 계절 속에 (Season of Memories)',
  '청혼하지 않을 이유를 못 찾았어',
  '인사',
  'LOVE DIVE',
  'Handlebars (feat. Dua Lipa)',
  'Live My Life',
  '사랑인가 봐',
  'After LIKE',
  '사건의 지평선',
  '헤어지자 말해요',
  'SPOT! (feat. JENNIE)',
  '미안해 미워해 사랑해',
  '청바지',
  '네모네모',
  'Boom Boom Bass',
  'ZEN',
  'LOVE ATTACK'],
 ['G-DRAGON',
  'KiiiKiii (키키)',
  'IVE (아이브)',
  'IVE (아이브)',
  'aespa',
  'WOODZ',
  'G-DRAGON',
  '황가람',
  'Hearts2Hearts (하츠투하츠)',
  'G-DRAGON',
  '황치열',
  '로제(ROSÉ)',
  '로제(ROSÉ)',
  '제니 (JENNIE)',
  '제니 (JENNIE)',
  'BOYNEXTDOOR',
  '조째즈',
  'DAY6 (데이식스)',
  'Lady GaGa(레이디 가가)',
  'Kendrick Lamar(켄드릭 라마)',
  'G-DRAGON',
  '지수(JISOO)',
  '제니 (JENNIE)',
  'BABYMONSTER',
  'aespa',
  '제니 (JENNIE)',
  'DAY6 (데이식스)',
  '재쓰비 (JAESSBEE)',
  '이무진',
  'G-DRAGON',
  '로이킴',
  'DAY6 (데이식스)',
  'aespa',
  '10CM',
  'QWER',
  'Kendrick Lamar(켄드릭 라마)',
  'QWER',
  'G-DRAGON',
  '황가람',
  '로제(ROSÉ)',
  'NewJeans',
  'NewJeans',
  '이클립스 (ECLIPSE)',
  'TWS (투어스)',
  '인피니트(Infinite)',
  '이창섭',
  '(여자)아이들',
  '이무진',
  'KISS OF LIFE',
  'aespa',
  'DAY6 (데이식스)',
  'DAY6 (데이식스)',
  'G-DRAGON',
  '아일릿(ILLIT)',
  'IVE (아이브)',
  'AKMU(악뮤)',
  'NewJeans',
  '아이유(IU)',
  'G-DRAGON',
  'NewJeans',
  '이영지',
  '폴킴(Paul Kim)',
  '프로미스나인',
  'RIIZE',
  '순순희 (지환)',
  'NewJeans',
  '비비(BIBI)',
  '성시경',
  '최유리',
  'DAY6 (데이식스)',
  'NewJeans',
  'KISS OF LIFE',
  '너드커넥션(Nerd Connection)',
  '태연 (TAEYEON)',
  'DAY6 (데이식스)',
  'IVE (아이브)',
  '임재현',
  '태연 (TAEYEON)',
  '잔나비',
  'NewJeans',
  'G-DRAGON',
  '아일릿(ILLIT)',
  'Taylor Swift(테일러 스위프트)',
  '여자친구(GFRIEND)',
  '이무진',
  '범진',
  'IVE (아이브)',
  '제니 (JENNIE)',
  'aespa',
  '멜로망스(MeloMance)',
  'IVE (아이브)',
  '윤하(Younha/ユンナ)',
  '박재정',
  '지코 (ZICO)',
  'Crush',
  '부석순(SEVENTEEN)',
  'YENA (최예나)',
  'RIIZE',
  '제니 (JENNIE)',
  'RESCENE (리센느)'])
</pre>

```python
year = 2025
month = 1
day = 1

titles, artists = bugs_music_week(year, month, day)

filename = './music_week.txt'

f = open(filename, 'w', encoding='utf-8')

for i in range(len(titles)):
    f.write("{}. {} - {}\n".format(i+1, titles[i], artists[i]))

f.close()
```


```python
url = "http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getUltraSrtNcst?"
service_key = 'serviceKey=t4yw1aXtpKNs18O08S%2BOc4PxBmJ7868FN6Lwqs%2FaxIlSJorYsuVVfYUKMOohf45ODRxHZIp1l%2B87Jpqcsnc%2Fgw%3D%3D'
others = '&numOfRows=10&pageNo=1&base_date=20250311&base_time=0600&nx=55&ny=127'

response = requests.get(url + service_key + others)
print(response.text)
```

<pre>
<?xml version="1.0" encoding="UTF-8"?>
<response><header><resultCode>00</resultCode><resultMsg>NORMAL_SERVICE</resultMsg></header><body><dataType>XML</dataType><items><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>PTY</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>REH</category><nx>55</nx><ny>127</ny><obsrValue>96</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>RN1</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>T1H</category><nx>55</nx><ny>127</ny><obsrValue>-1.3</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>UUU</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>VEC</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>VVV</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>WSD</category><nx>55</nx><ny>127</ny><obsrValue>0.1</obsrValue></item></items><numOfRows>10</numOfRows><pageNo>1</pageNo><totalCount>8</totalCount></body></response>

</pre>

```python
url = 'http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getUltraSrtNcst'

params = {
    "serviceKey" : "t4yw1aXtpKNs18O08S+Oc4PxBmJ7868FN6Lwqs/axIlSJorYsuVVfYUKMOohf45ODRxHZIp1l+87Jpqcsnc/gw==",
    "numOfRows" : 10,
    "pageNo" : 1,
    "base_date" : "20250311",
    "base_time" : "0600",
    "nx" : 55,
    "ny" : 127,
    "dataType" : "XML"
}

response = requests.get(url, params=params)
print(response.text)
```

<pre>
<?xml version="1.0" encoding="UTF-8"?>
<response><header><resultCode>00</resultCode><resultMsg>NORMAL_SERVICE</resultMsg></header><body><dataType>XML</dataType><items><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>PTY</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>REH</category><nx>55</nx><ny>127</ny><obsrValue>96</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>RN1</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>T1H</category><nx>55</nx><ny>127</ny><obsrValue>-1.3</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>UUU</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>VEC</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>VVV</category><nx>55</nx><ny>127</ny><obsrValue>0</obsrValue></item><item><baseDate>20250311</baseDate><baseTime>0600</baseTime><category>WSD</category><nx>55</nx><ny>127</ny><obsrValue>0.1</obsrValue></item></items><numOfRows>10</numOfRows><pageNo>1</pageNo><totalCount>8</totalCount></body></response>

</pre>

```python
import xml.etree.ElementTree as ET # XML 데이터를 파싱하는 라이브러리
from datetime import datetime, timedelta # 날짜, 시간 처리를 하는 라이브러리
```


```python
def fetch_weather_data(service_key, base_date, base_time, nx, ny):
    url = 'http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getUltraSrtNcst'
    params = {
        "serviceKey" : service_key,
        "numOfRows" : 10,
        "pageNo" : 1,
        "base_date" : base_date,
        "base_time" : base_time,
        "nx" : nx,
        "ny" : ny,
        "dataType" : "XML"
    }
    
    response = requests.get(url, params = params)
    
    if respones.status_code ==200:
        try:
            xml_data = ET.fromstring(response.content)
            return xml_data
            
        except ET.ParseError:
            print("Error parsing XML response, Content :" , reponse.content)
            return None
        
        else:
            print(response.status_code)
```


```python
def parse_weather_data(xml_data):
    if xml_data is None:
        return []

    header = xml_data.find('header')
    if header is not None:
        result_code = header.find('resultCode').text
        result_msg = header.find('resultMsg').text

        if result_code != '00':
            print(f'API ERROR : {result_msg} (CODE : {result_code})')
            return []

    body = xml_data.find('body')
    if body is None:
        print("No body found in XML response.")
        return []

    items = body.find('items')
    if items is None:
        print("No items found in XML response.")
        return []

    item_list = items.findall('item')
    data = []

    for item in item_list:
        data.append({
            "baseDate" : item.find("baseDate").text,
            "baseTime" : item.find("baseTime").text,
            "category" : item.find("category").text,
            "obsrValue" : item.find("obsrValue").text
        })

    return data
```


```python
def run_weather_data(service_key, start_date, days, nx, ny):
    weather_data = []
    date = datetime.strptime(start_date, "%Y%m%d") # start_date를 %Y%m%d의 형식으로 변환 (문자열 -> 객체)

    for day in range(days):
        for hour in ["0200", "0500", "0800", "1100", "1300", "1600"]:
            base_date = date.strftime("%Y%m%d") # format (객체 -> 문자열)
            xml_data = fetch_weather_data(service_key, base_date, hour, nx, ny)
            if xml_data is not None:
                day_data = parse_weather_data(xml_data)
                weather_data.extend(day_data)
            else :
                print(f'No data returned for {base_date} - {hour}')
            
        date += timedelta(days=1)

    return weather_data
```


```python
service_key = 'YbuTM45CQEwlChbMzwt1%2BzLdiwAPIJmq9pnZgivZR9BMg7xDTWf291IV5oA6ur6onuWSm61VUNnh2WTUX9pcrQ%3D%3D'
days = 1
nx = 55
ny = 127
weather_data = run_weather_data(service_key, start_date, days, nx, ny)
weather_data
```


```python
```


```python
import pandas as pd

df = pd.DataFrame(weather_data)

df['baseDateTime'] = pd.to_datetime(df['baseDate'] + df['baseTime'], format='%Y%m%d%H%M')

# Pandas 데이터를 재구성할 때 사용된다. pivot, pivot_table(중복 처리)
df_pivot = df.pivot(index='baseDateTime', columns='category', values='obsrValue')
df_pivot
```


```python
df = pd.DataFrame(weather_data)

df['baseDateTime'] = pd.to_datetime(df['baseDate'] + df['baseTime'], format='%Y%m%d%H%M')

df['obsrValue'] = pd.to_numeric(df['obsrValue']) # 문자열 -> 숫자
# pivot_table(중복 처리) : # aggfunc = mean 기본값
df_pivot = df.pivot_table(index='baseDateTime', columns='category', values='obsrValue', aggfunc='mean') 
df_pivot
```


```python
!pip install selenium
```

<pre>
zsh:1: command not found: pip
</pre>

```python
from selenium import
```
